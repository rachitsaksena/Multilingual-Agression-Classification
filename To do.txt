ENGLISH-TRAIN:
1 - Bias Regularization:
> Baseline
> Effect of upsampling
> Effect of downsampling
> Weighted classification

2 - Cleaning and Pre-Embedding EDA:
> Remove corpus specific stop words
> Class based Interdependence
> Lemmatization

3 - Fine-tuning:
> t-SNE perplexity vals
> BOW, TFIDF (min, max df)
> LDA vecs (after pre embedding cleaning) - test baseline as well
> Word2Vec(with/without shuffle, avg vs tfidf) (pre trained and custom)
> GloVe (window, epochs, etc) (pre trained and custom)
> FastText (pre trained and custom)

4 - Meta Feature Engineering

5 - Bugs:
> Hug emoji
> Punctuations (!, etc)
> Numbers
> Bad hyperlink cleaning (httpsyoutbe)
> Bad transliteration dict (Use Sets)
> Bad spelling corrections {couture (is already a word) instead of culture}
> Bad language tagging and translation (some Bangla got through)


Left overs:
> CoreNLP multilingual POS Tags
> Lang generalized algos
> Implement Adversarial Validation before EDA and use implemented Algos to generate Features.
> Model Training and Classification (ENSEMBLE)
> Transfer Learning Fresh Notebook

Ideas?
> Other sentence embeddings
1. Doc2Vec
2. SentenceBERT
3. InferSent
4. Universal Sentence Encoder

