{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas                       as pd\n",
    "import numpy                        as np\n",
    "import torch\n",
    "import torch.nn                     as nn\n",
    "import torch.nn.functional          as F\n",
    "import torch.optim                  as optim\n",
    "from torchtext                      import data\n",
    "import torchtext\n",
    "import re\n",
    "from sklearn.metrics                import roc_auc_score\n",
    "from sklearn.metrics                import roc_curve, auc\n",
    "import matplotlib.pyplot            as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ramanshgrover/anaconda3/lib/python3.6/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/Users/ramanshgrover/anaconda3/lib/python3.6/site-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(tokenize='spacy',batch_first=True,include_lengths=True, lower = True, )\n",
    "LABEL = data.LabelField(dtype = torch.float,batch_first=True, lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "      <th>B</th>\n",
       "      <th>clean</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>normalized lexicon</th>\n",
       "      <th>monolingual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>C45.451</td>\n",
       "      <td>Next part</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "      <td>next part</td>\n",
       "      <td>['next', 'part']</td>\n",
       "      <td>['next', 'part']</td>\n",
       "      <td>next part</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>C47.11</td>\n",
       "      <td>Iii8mllllllm\\nMdxfvb8o90lplppi0005</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "      <td>iii8mllllllm mdxfvb8o90lplppi0005</td>\n",
       "      <td>['iii', '8mllllllm', 'mdxfvb', '8o90lplppi0005']</td>\n",
       "      <td>['iii', '8mllllllm', 'mdxfvb', '8o90lplppi0005']</td>\n",
       "      <td>iii 8mllllllm mdxfvb 8o90lplppi0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>C33.79</td>\n",
       "      <td>ü§£ü§£üòÇüòÇü§£ü§£ü§£üòÇosm vedio ....keep it up...make more v...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "      <td>osm vedio make vedios</td>\n",
       "      <td>['osm', 'vedio', 'make', 'vedios']</td>\n",
       "      <td>['osm', 'osf', 'vedic', 'make', 'videos']</td>\n",
       "      <td>osm osf vedic make videos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>C4.1961</td>\n",
       "      <td>What the fuck was this? I respect shwetabh and...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "      <td>what fuck this? respect shwetabh watching vide...</td>\n",
       "      <td>['what', 'fuck', 'this', '?', 'respect', 'shwe...</td>\n",
       "      <td>['what', 'fuck', 'fuck', 'this', '?', 'respect...</td>\n",
       "      <td>what fuck fuck this ? respect respect whitish ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>C10.153</td>\n",
       "      <td>Concerned authorities should bring arundathi R...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "      <td>concerned authorities bring arundathi roy type...</td>\n",
       "      <td>['concerned', 'authorities', 'bring', 'arundat...</td>\n",
       "      <td>['concerned', 'authorities', 'bring', 'arundat...</td>\n",
       "      <td>concerned authorities bring arundathi roy roy ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       ID                                               Text  \\\n",
       "0           0  C45.451                                          Next part   \n",
       "1           1   C47.11                 Iii8mllllllm\\nMdxfvb8o90lplppi0005   \n",
       "2           2   C33.79  ü§£ü§£üòÇüòÇü§£ü§£ü§£üòÇosm vedio ....keep it up...make more v...   \n",
       "3           3  C4.1961  What the fuck was this? I respect shwetabh and...   \n",
       "4           4  C10.153  Concerned authorities should bring arundathi R...   \n",
       "\n",
       "  label     B                                              clean  \\\n",
       "0   NAG  NGEN                                          next part   \n",
       "1   NAG  NGEN                  iii8mllllllm mdxfvb8o90lplppi0005   \n",
       "2   NAG  NGEN                              osm vedio make vedios   \n",
       "3   NAG  NGEN  what fuck this? respect shwetabh watching vide...   \n",
       "4   NAG  NGEN  concerned authorities bring arundathi roy type...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0                                   ['next', 'part']   \n",
       "1   ['iii', '8mllllllm', 'mdxfvb', '8o90lplppi0005']   \n",
       "2                 ['osm', 'vedio', 'make', 'vedios']   \n",
       "3  ['what', 'fuck', 'this', '?', 'respect', 'shwe...   \n",
       "4  ['concerned', 'authorities', 'bring', 'arundat...   \n",
       "\n",
       "                                  normalized lexicon  \\\n",
       "0                                   ['next', 'part']   \n",
       "1   ['iii', '8mllllllm', 'mdxfvb', '8o90lplppi0005']   \n",
       "2          ['osm', 'osf', 'vedic', 'make', 'videos']   \n",
       "3  ['what', 'fuck', 'fuck', 'this', '?', 'respect...   \n",
       "4  ['concerned', 'authorities', 'bring', 'arundat...   \n",
       "\n",
       "                                         monolingual  \n",
       "0                                          next part  \n",
       "1                iii 8mllllllm mdxfvb 8o90lplppi0005  \n",
       "2                          osm osf vedic make videos  \n",
       "3  what fuck fuck this ? respect respect whitish ...  \n",
       "4  concerned authorities bring arundathi roy roy ...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './Data/cleaned english.csv'\n",
    "df = pd.read_csv(path)\n",
    "df.rename(columns = {'Unnamed: 0': 'Unnamed: 0', 'ID': 'ID', 'Text': 'Text', 'Sub-task A': 'label', 'Sub-task B': 'B', 'clean text': 'clean', 'tokenized': 'tokenized', 'normalized lexicon': 'normalized lexicon', 'monolingual': 'monolingual'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "\n",
    "for i in range(len(df)):\n",
    "    x = df.monolingual.iloc[i]\n",
    "    if len(x) <= 0:\n",
    "        print(df.monolingual.iloc[i])\n",
    "        df.monolingual.iloc[i] = None\n",
    "        \n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                               next part\n",
       "1                     iii 8mllllllm mdxfvb 8o90lplppi0005\n",
       "2                               osm osf vedic make videos\n",
       "3       what fuck fuck this ? respect respect whitish ...\n",
       "4       concerned authorities bring arundathi roy roy ...\n",
       "5                           famous care die hateful talks\n",
       "6                            best best topic law students\n",
       "7       even kabir kabir singh singh unaware preeti so...\n",
       "8                                         she wrong wrong\n",
       "9                                              6001733614\n",
       "10                                      hindi hindi movie\n",
       "11                                                   wait\n",
       "12                                     very totally agree\n",
       "14      ushant said exactly ! movie movie comprehend d...\n",
       "15                     our role model whitish kabir kabir\n",
       "16      usher luck luck haryana haryana small city cal...\n",
       "17      arbitrator kamatipura sonagachi area area sett...\n",
       "18      yeah man man * * fuck fuck bollywood hollywood...\n",
       "19      calls uneducated person means unexpected behav...\n",
       "20        sahi shai logic level - - - bollywood hollywood\n",
       "21                        mind keeda need gay gay lesbian\n",
       "22      giving movie movie review review super super 30 ?\n",
       "23      sarah dived sir sir appreciate maintaining neu...\n",
       "24      why indian indian government actions talk nati...\n",
       "25      exactly that 10000000000 x shitty shitty kabir...\n",
       "26                                  shallow review review\n",
       "27                                 mohammad ismail ismail\n",
       "28      feminism feminism means equality discriminatio...\n",
       "29                                          like bahubali\n",
       "30      blockbuster kabir kabir singh singh yes like c...\n",
       "                              ...                        \n",
       "4233          spoiler alert alert movie movie ruined save\n",
       "4234                              red top journalist hats\n",
       "4235                                 sail channel liberal\n",
       "4236                                                agree\n",
       "4237                                              osm osf\n",
       "4238    personality beggar lasts heroin laugh rich ran...\n",
       "4239     thank sir sir real showing truth respect respect\n",
       "4240                       ho top gay gay call 9876380919\n",
       "4241                                               supper\n",
       "4242                      salute salute jahangir jahangir\n",
       "4243                   absolutely hell hell india india !\n",
       "4244                               totally agree thoughts\n",
       "4245    hours fucker mistake video video list fully il...\n",
       "4246    nil nil the captain captain war war terrible g...\n",
       "4247    sachin sakri sari arjun arjun reddy reddy fuck...\n",
       "4248                                                  men\n",
       "4249                                       nice nice boos\n",
       "4250    pratik ratio bhai thai watch watch aravinda ar...\n",
       "4251    sir sir make video video nato death note anima...\n",
       "4252                                       nice nice boss\n",
       "4253                             ban sued exists liberals\n",
       "4254                                  open bob bob wagane\n",
       "4255    with due respect respect sir sir agree hope op...\n",
       "4256    sardara sikhs respect respect patriotism prote...\n",
       "4257                       why youtube couture ads chanel\n",
       "4258    abbey loudly arnab arab did videos eunuch poli...\n",
       "4259          arundati rich money paying depressed senile\n",
       "4260    people criticize pratik ratio borade takes mov...\n",
       "4261                                                nawaz\n",
       "4262                     read book sir sir make available\n",
       "Name: monolingual, Length: 4205, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['monolingual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ramanshgrover/anaconda3/lib/python3.6/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n",
      "/Users/ramanshgrover/anaconda3/lib/python3.6/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'nag', 'monolingual': ['what', 'fuck', 'fuck', 'this', '?', 'respect', 'respect', 'whitish', 'watching', 'videos', 'videos', 'long', 'time', 'time', 'review', 'review', 'shit', 'shit', 'nobody', 'watch', 'watch', 'movie', 'movie', 'reality', 'blah', 'you', 'watch', 'watch', 'movies', 'inspiration', 'read', 'books', 'biographies', 'yes', 'influenced', 'shitty', 'shitty', 'fucker', 'anyone', 'takes', 'movie', 'movie', 'seriously', 'fucker', 'and', 'theatre', 'theatre', 'india', 'india', 'reality', 'shit', 'shit', 'seek', 'reality', 'logical', 'scripts', 'characters', 'potholes', 'watching', 'kabir', 'kabir', 'singh', 'singh', 'made', 'views', 'called', 'raw', 'review', 'review', 'and', 'acted', 'forgot', 'movie', 'movie', 'story', 'story', 'pronounced', 'overtype', 'mess', 'wrongly', '?', 'common', '!']}\n"
     ]
    }
   ],
   "source": [
    "fields = [(None, None), (None, None), (None, None), ('label', LABEL), (None, None), (None, None), (None, None), (None, None), ('monolingual', TEXT)]\n",
    "training_data=data.TabularDataset(path = path, format = 'csv', fields = fields, skip_header = True)\n",
    "\n",
    "print(vars(training_data.examples[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = training_data.split(split_ratio=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = torchtext.vocab.Vectors('wiki-news-300d-1M.vec', cache = './Cache/Embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of topic vocab: 2012\n",
      "Size of label vocab: 3\n",
      "[('movie', 816), ('kabir', 494), ('singh', 431), ('video', 375), ('review', 364), ('?', 297), ('india', 273), ('nice', 259), ('man', 256), ('watch', 237), ('sir', 232)]\n",
      "[('nag', 2373), ('cag', 316), ('oag', 295)]\n",
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x11ff90160>>, {'<unk>': 0, '<pad>': 1, 'movie': 2, 'kabir': 3, 'singh': 4, 'video': 5, 'review': 6, '?': 7, 'india': 8, 'nice': 9, 'man': 10, 'watch': 11, 'sir': 12, 'a': 13, 'love': 14, 'character': 15, 'time': 16, 'great': 17, '!': 18, 'wrong': 19, 'you': 20, 'feminism': 21, 'the': 22, '\"': 23, 'indian': 24, 't': 25, 'god': 26, 'hollywood': 27, 'this': 28, 'roy': 29, '-': 30, 'movies': 31, 'arundhati': 32, 'feminist': 33, 'story': 34, 'and': 35, 'respect': 36, '=': 37, 'film': 38, '*': 39, 'bollywood': 40, 'brother': 41, 'arjun': 42, 'life': 43, 'agree': 44, 's': 45, 'good': 46, 'guy': 47, 'she': 48, 'make': 49, 'reddy': 50, 'couture': 51, 'fuck': 52, 'youtube': 53, 'girl': 54, 'gay': 55, 'shit': 56, 'videos': 57, 'view': 58, 'women': 59, 'channel': 60, 'feminists': 61, 'joker': 62, 'right': 63, 'shahid': 64, '%': 65, 'but': 66, 'world': 67, 'super': 68, 'men': 69, 'liberal': 70, \"'\": 71, 'bro': 72, 'v': 73, 'that': 74, 'person': 75, 'what': 76, 'best': 77, 'army': 78, 'country': 79, 'lady': 80, 'real': 81, 'its': 82, 'problem': 83, 'give': 84, 'ibm': 85, 'they': 86, 'hero': 87, 'liberals': 88, 'support': 89, 'point': 90, 'govt': 91, 'watching': 92, '#': 93, 'pratik': 94, 'ratio': 95, 'stupid': 96, 'law': 97, 'made': 98, 'understand': 99, 'girls': 100, 'totally': 101, 'big': 102, 'do': 103, 'indians': 104, 'reviews': 105, 'sex': 106, 'why': 107, 'woman': 108, 'father': 109, 'guys': 110, '&': 111, 'male': 112, 'please': 113, 'speak': 114, 'very': 115, 'watched': 116, 'called': 117, 'kind': 118, 'modi': 119, 'ran': 120, 'arab': 121, 'society': 122, 'act': 123, 'amazing': 124, 'awesome': 125, 'bhai': 126, 'fact': 127, 'thai': 128, 'arnab': 129, 'hate': 130, 'anti': 131, 'not': 132, 'pakistan': 133, 'ranu': 134, 'there': 135, 'things': 136, 'generation': 137, 'job': 138, 'salute': 139, 'superb': 140, 'acting': 141, 'comresults': 142, 'search_query': 143, 'talk': 144, 'yes': 145, 'call': 146, 'coldwatch': 147, 'english': 148, 'follow': 149, 'free': 150, 'kapoor': 151, 'opinion': 152, 'comment': 153, 'debate': 154, 'family': 155, 'making': 156, 'people': 157, 'social': 158, 'thanks': 159, 'feel': 160, 'show': 161, '<': 162, '>': 163, 'boy': 164, 'court': 165, 'nrc': 166, 'thank': 167, 'characters': 168, 'how': 169, 'national': 170, 'ppl': 171, 'reply': 172, 'shirt': 173, 'well': 174, 'agenda': 175, 'bless': 176, 'dada': 177, 'don': 178, 'jahangir': 179, 'live': 180, 'mad': 181, \"n't\": 182, 'points': 183, 'rights': 184, 'truth': 185, 'first': 186, 'httpsyoutu': 187, 'put': 188, 'state': 189, 'stop': 190, 'these': 191, 'violence': 192, 'work': 193, 'absolutely': 194, 'answer': 195, 'end': 196, 'face': 197, 'female': 198, 'just': 199, 'lesbian': 200, 'mind': 201, 'now': 202, 'preeti': 203, 'scene': 204, 'south': 205, 'talking': 206, 'year': 207, 'your': 208, '‡§æ': 209, \"'s\": 210, 'all': 211, 'back': 212, 'different': 213, 'kumar': 214, 'loved': 215, 'one': 216, 'pseudo': 217, 'reality': 218, 'waiting': 219, 'who': 220, 'wow': 221, 'caa': 222, 'fake': 223, 'finally': 224, 'friend': 225, 'government': 226, 'hindi': 227, 'honest': 228, 'it': 229, 'message': 230, 'plc': 231, 'plz': 232, 'positive': 233, 'read': 234, 'toxic': 235, 'are': 236, 'companion': 237, 'future': 238, 'long': 239, 'lot': 240, 'means': 241, 'part': 242, 'range': 243, 'relationship': 244, 'shown': 245, 'views': 246, 'whitish': 247, 'bjp': 248, 'college': 249, 'congress': 250, 'day': 251, 'equality': 252, 'his': 253, 'lol': 254, 'news': 255, 'police': 256, 'sense': 257, 'shitty': 258, 'sucharita': 259, 'telugu': 260, 'thoughts': 261, 'western': 262, 'when': 263, '100': 264, 'at': 265, 'boss': 266, 'boys': 267, 'correct': 268, 'gender': 269, 'media': 270, 'money': 271, 'negative': 272, 'years': 273, 'youth': 274, 'am': 275, 'behavior': 276, 'bullshit': 277, 'change': 278, 'culture': 279, 'false': 280, 'for': 281, 'fucker': 282, 'giving': 283, 'hell': 284, 'hindus': 285, 'human': 286, 'iam': 287, 'nt': 288, 'true': 289, 'urban': 290, 'bill': 291, 'constitution': 292, 'films': 293, 'find': 294, 'homosexual': 295, 'homosexuality': 296, 'idiot': 297, 'left': 298, 'like': 299, 'marriage': 300, 'masculinity': 301, 'shwetabh': 302, 'away': 303, 'bitch': 304, 'case': 305, 'chutiya': 306, 'concept': 307, 'crap': 308, 'created': 309, 'disgusting': 310, 'dislikes': 311, 'fucking': 312, 'hai': 313, 'hope': 314, 'issues': 315, 'judge': 316, 'only': 317, 'parents': 318, 'perfect': 319, 'proud': 320, 'randi': 321, 'reason': 322, 'shame': 323, 'start': 324, 'times': 325, 'war': 326, '\\u200d': 327, '.': 328, '498a': 329, 'allowed': 330, 'billa': 331, 'care': 332, 'crime': 333, 'did': 334, 'dog': 335, 'friends': 336, 'is': 337, 'mandal': 338, 'mental': 339, 'nature': 340, 'public': 341, 'r98': 342, 'ranga': 343, 'shows': 344, 'type': 345, 'word': 346, 'analysis': 347, 'audience': 348, 'beautiful': 349, 'behaviour': 350, 'book': 351, 'can': 352, 'control': 353, 'cool': 354, 'dis': 355, 'fan': 356, 'funny': 357, 'happy': 358, 'hard': 359, 'hindu': 360, 'home': 361, 'kung': 362, 'likes': 363, 'luck': 364, 'masterpiece': 365, 'modern': 366, 'nation': 367, 'pls': 368, 'reviewer': 369, 'send': 370, 'sexual': 371, 'sick': 372, 'single': 373, 'speech': 374, 'today': 375, 'wanted': 376, 'whatsapp': 377, 'abuse': 378, 'appreciate': 379, 'biggest': 380, 'body': 381, 'cinema': 382, 'dear': 383, 'destructive': 384, 'equal': 385, 'freedom': 386, 'garbage': 387, 'kasturi': 388, 'learn': 389, 'least': 390, 'pasture': 391, 'really': 392, 'share': 393, 'side': 394, 'situation': 395, 'sorry': 396, 'speaking': 397, 'will': 398, '0': 399, 'atleast': 400, 'changed': 401, 'coming': 402, 'completely': 403, 'coz': 404, 'difference': 405, 'even': 406, 'every': 407, 'hit': 408, 'house': 409, 'instead': 410, 'intellectual': 411, 'jail': 412, 'justify': 413, 'lgb': 414, 'lgbt': 415, 'nupur': 416, 'of': 417, 'open': 418, 'perspective': 419, 'play': 420, 'rape': 421, 'rss': 422, 'sharma': 423, 'showing': 424, 'tamil': 425, 'wife': 426, '+': 427, 'agreed': 428, 'attitude': 429, 'based': 430, 'days': 431, 'delhi': 432, 'deserve': 433, 'director': 434, 'genuine': 435, 'husband': 436, 'industry': 437, 'jai': 438, 'language': 439, 'laws': 440, 'legal': 441, 'lost': 442, 'n_zmfqmzos': 443, 'pretty': 444, 'problems': 445, 'realistic': 446, 'sanders': 447, 'shots': 448, 'simple': 449, 'some': 450, 'team': 451, 'till': 452, 'wedding': 453, 'accept': 454, 'actor': 455, 'alcoholic': 456, 'alert': 457, 'born': 458, 'bring': 459, 'consent': 460, 'content': 461, 'copy': 462, 'dangerous': 463, 'devil': 464, 'didi': 465, 'domestic': 466, 'dowry': 467, 'due': 468, 'emotions': 469, 'enjoy': 470, 'felt': 471, 'get': 472, 'go': 473, 'hear': 474, 'her': 475, 'i': 476, 'immediately': 477, 'information': 478, 'issue': 479, 'keep': 480, 'leave': 481, 'm': 482, 'makes': 483, 'married': 484, 'most': 485, 'move': 486, 'msg': 487, 'nailed': 488, 'nor': 489, 'number': 490, 'o': 491, 'opinions': 492, 'population': 493, 'sardar': 494, 'section': 495, 'series': 496, 'song': 497, 'stay': 498, 'such': 499, 'supreme': 500, 'telling': 501, 'usa': 502, 'vai': 503, 'wanna': 504, 'with': 505, 'words': 506, 'worst': 507, 'ü§ó': 508, '377': 509, 'actors': 510, 'after': 511, 'believe': 512, 'bloody': 513, 'china': 514, 'course': 515, 'drugs': 516, 'dumb': 517, 'gays': 518, 'journalist': 519, 'khan': 520, 'kiss': 521, 'knew': 522, 'level': 523, 'meaning': 524, 'mr': 525, 'natural': 526, 'night': 527, 'npr': 528, 'outta': 529, 'personal': 530, 'place': 531, 'poor': 532, 'possible': 533, 'rajdeep': 534, 'shut': 535, 'sister': 536, 'spreading': 537, 'stand': 538, 'straight': 539, 'top': 540, 'voice': 541, 'was': 542, 'waste': 543, \"you'se\": 544, '‡¶Ü': 545, 'action': 546, 'aha': 547, 'anger': 548, 'any': 549, 'arrested': 550, 'ass': 551, 'bangladesh': 552, 'bastard': 553, 'bed': 554, 'bit': 555, 'check': 556, 'club': 557, 'damn': 558, 'dark': 559, 'death': 560, 'doctor': 561, 'drama': 562, 'drinking': 563, 'emotional': 564, 'everyone': 565, 'excellent': 566, 'explanation': 567, 'eye': 568, 'guess': 569, 'hand': 570, 'heroine': 571, 'homosexuals': 572, 'hot': 573, 'idiots': 574, 'imagine': 575, 'lead': 576, 'lots': 577, 'moron': 578, 'mouth': 579, 'new': 580, 'north': 581, 'our': 582, 'prize': 583, 'rapist': 584, 'rat': 585, 'ready': 586, 'responsibility': 587, 'same': 588, 'shah': 589, 'shoes': 590, 'slap': 591, 'smart': 592, 'soo': 593, 'suffer': 594, 'understanding': 595, 'up': 596, 'values': 597, 'yar': 598, 'alcohol': 599, 'also': 600, 'anima': 601, 'arundati': 602, 'asked': 603, 'bible': 604, 'borade': 605, 'child': 606, 'clear': 607, 'comments': 608, 'common': 609, 'countries': 610, 'deepika': 611, 'earth': 612, 'entertainment': 613, 'expect': 614, 'f': 615, 'feeling': 616, 'flawed': 617, 'fu': 618, 'full': 619, 'gandhi': 620, 'gave': 621, 'hats': 622, 'high': 623, 'husbands': 624, 'ideas': 625, 'killed': 626, 'kutta': 627, 'leftist': 628, 'let': 629, 'lie': 630, 'logic': 631, 'madam': 632, 'mensural': 633, 'moral': 634, 'more': 635, 'name': 636, 'need': 637, 'no': 638, 'non': 639, 'openly': 640, 'opposite': 641, 'order': 642, 'peoples': 643, 'political': 644, 'promote': 645, 'released': 646, 'request': 647, 'sad': 648, 'scenes': 649, 'seriously': 650, 'slapped': 651, 'songs': 652, 'started': 653, 'stopped': 654, 'su': 655, 'takes': 656, 'taking': 657, 'taxi': 658, 'told': 659, 'typical': 660, 'vijay': 661, 'wish': 662, 'yaar': 663, 'young': 664, '~': 665, '‡•ç': 666, '\\\\': 667, 'actions': 668, 'arrest': 669, 'bad': 670, 'basic': 671, 'because': 672, 'boycott': 673, 'bravo': 674, 'break': 675, 'breakup': 676, 'broken': 677, 'calling': 678, 'cheap': 679, 'choice': 680, 'chopra': 681, 'class': 682, 'direction': 683, 'directors': 684, 'dogs': 685, 'dude': 686, 'education': 687, 'entire': 688, 'especially': 689, 'explain': 690, 'fans': 691, 'forget': 692, 'found': 693, 'gang': 694, 'grey': 695, 'group': 696, 'grow': 697, 'happen': 698, 'happened': 699, 'heroic': 700, 'hii': 701, 'huge': 702, 'ie': 703, 'illiterate': 704, 'injustice': 705, 'innocent': 706, 'inside': 707, 'interested': 708, 'intro': 709, 'kashmir': 710, 'line': 711, 'list': 712, 'literally': 713, 'living': 714, 'loves': 715, 'medical': 716, 'mensutra': 717, 'mislead': 718, 'misogyny': 719, 'monday': 720, 'multiple': 721, 'muslim': 722, 'needed': 723, 'nonsense': 724, 'normal': 725, 'nothing': 726, 'office': 727, 'omg': 728, 'pain': 729, 'partner': 730, 'pathetic': 731, 'personally': 732, 'physical': 733, 'played': 734, 'politics': 735, 'power': 736, 'road': 737, 'role': 738, 'said': 739, 'shankar': 740, 'study': 741, 'system': 742, 'thankyou': 743, 'then': 744, 'threat': 745, 'topic': 746, 'topper': 747, 'tyagi': 748, 'u': 749, 'write': 750, 'youtuber': 751, '_': 752, 'abusive': 753, 'ache': 754, 'addict': 755, 'age': 756, 'area': 757, 'arundathi': 758, 'b': 759, 'baby': 760, 'basically': 761, 'beard': 762, 'being': 763, 'better': 764, 'black': 765, 'blood': 766, 'box': 767, 'brain': 768, 'cancer': 769, 'citizenship': 770, 'complete': 771, 'consider': 772, 'continue': 773, 'criminal': 774, 'debates': 775, 'deep': 776, 'depends': 777, 'destroyed': 778, 'dint': 779, 'everything': 780, 'exactly': 781, 'expecting': 782, 'facts': 783, 'fault': 784, 'fight': 785, 'from': 786, 'fun': 787, 'half': 788, 'hated': 789, 'have': 790, 'hello': 791, 'here': 792, 'hey': 793, 'hypocrisy': 794, 'ignorant': 795, 'individual': 796, 'influenced': 797, 'inspiration': 798, 'j': 799, 'knife': 800, 'know': 801, 'lies': 802, 'listen': 803, 'lives': 804, 'logical': 805, 'marry': 806, 'matter': 807, 'members': 808, 'mentally': 809, 'misandrists': 810, 'misleading': 811, 'months': 812, 'mother': 813, 'nec': 814, 'nyc': 815, 'osf': 816, 'osm': 817, 'paid': 818, 'partners': 819, 'peace': 820, 'performance': 821, 'pleasure': 822, 'plzz': 823, 'promoting': 824, 'prostitute': 825, 'random': 826, 'religion': 827, 'remember': 828, 'rock': 829, 'rubbish': 830, 'run': 831, 'ryt': 832, 'save': 833, 'short': 834, 'slapping': 835, 'so': 836, 'sort': 837, 'spoiler': 838, 'spread': 839, 'stars': 840, 'starting': 841, 'step': 842, 'student': 843, 'stuff': 844, 'suicide': 845, 'supporting': 846, 'take': 847, 'teach': 848, 'their': 849, 'thinks': 850, 'total': 851, 'trailer': 852, 'ugly': 853, 'value': 854, 'vedas': 855, 'viewers': 856, 'wonder': 857, 'wwf': 858, '‡¶è': 859, '1st': 860, 'about': 861, 'absolute': 862, 'abusing': 863, 'accepted': 864, 'activities': 865, 'acts': 866, 'actual': 867, 'actually': 868, 'ago': 869, 'asshole': 870, 'available': 871, 'banned': 872, 'barman': 873, 'bars': 874, 'bashing': 875, 'bc': 876, 'bcoz': 877, 'bcz': 878, 'behave': 879, 'beings': 880, 'bitches': 881, 'blowing': 882, 'books': 883, 'boz': 884, 'bull': 885, 'burn': 886, 'c': 887, 'cares': 888, 'census': 889, 'citizens': 890, 'committed': 891, 'community': 892, 'cut': 893, 'decent': 894, 'defend': 895, 'democracy': 896, 'developed': 897, 'does': 898, 'dum': 899, 'ending': 900, 'expose': 901, 'fabulous': 902, 'fantastic': 903, 'feelings': 904, 'fiction': 905, 'fighting': 906, 'file': 907, 'final': 908, 'forcing': 909, 'glad': 910, 'glorifying': 911, 'gourami': 912, 'gupta': 913, 'happening': 914, 'harsh': 915, 'heaven': 916, 'hind': 917, 'honestly': 918, 'hypocrite': 919, 'idea': 920, 'ill': 921, 'illegal': 922, 'illogical': 923, 'impact': 924, 'in': 925, 'inspire': 926, 'interest': 927, 'islam': 928, 'item': 929, 'join': 930, 'joke': 931, 'kissing': 932, 'knowledge': 933, 'krishna': 934, 'kungfu': 935, 'lesbians': 936, 'liberalism': 937, 'lord': 938, 'lover': 939, 'loving': 940, 'lutyens': 941, 'major': 942, 'may': 943, 'meant': 944, 'minded': 945, 'mission': 946, 'na': 947, 'naam': 948, 'nam': 949, 'named': 950, 'others': 951, 'pak': 952, 'pakistani': 953, 'particular': 954, 'personality': 955, 'portray': 956, 'ppls': 957, 'process': 958, 'program': 959, 'propaganda': 960, 'protagonist': 961, 'publicity': 962, 'punished': 963, 'punjabi': 964, 'purpose': 965, 'question': 966, 'raja': 967, 'raw': 968, 'reaction': 969, 'realize': 970, 'remake': 971, 'responsible': 972, 'rest': 973, 'sanjo': 974, 'screen': 975, 'self': 976, 'shouting': 977, 'showed': 978, 'simply': 979, 'since': 980, 'singer': 981, 'sound': 982, 'st': 983, 'starts': 984, 'stories': 985, 'submissive': 986, 'subscribed': 987, 'subscribers': 988, 'sugar': 989, 'surgeon': 990, 'think': 991, 'those': 992, 'too': 993, 'uncle': 994, 'universe': 995, 'valid': 996, 'valley': 997, 'van': 998, 'vere': 999, 'vital': 1000, 'wave': 1001, 'west': 1002, 'where': 1003, 'worth': 1004, 'written': 1005, 'yea': 1006, 'yesterday': 1007, '\\U0001f92d': 1008, '09777070288': 1009, '30': 1010, 'accent': 1011, 'according': 1012, 'address': 1013, 'affected': 1014, 'anchor': 1015, 'angry': 1016, 'anymore': 1017, 'arguments': 1018, 'ban': 1019, 'bastards': 1020, 'bear': 1021, 'beware': 1022, 'bhubaneswar': 1023, 'bight': 1024, 'bisht': 1025, 'blockbuster': 1026, 'bottom': 1027, 'brought': 1028, 'build': 1029, 'business': 1030, 'cases': 1031, 'charecter': 1032, 'close': 1033, 'come': 1034, 'comedy': 1035, 'consequences': 1036, 'contact': 1037, 'core': 1038, 'crazy': 1039, 'create': 1040, 'criminals': 1041, 'critics': 1042, 'current': 1043, 'cute': 1044, 'dead': 1045, 'definitely': 1046, 'depression': 1047, 'deserves': 1048, 'development': 1049, 'deviation': 1050, 'dialogues': 1051, 'dick': 1052, 'didn': 1053, 'die': 1054, 'disagree': 1055, 'disappointed': 1056, 'dish': 1057, 'dna': 1058, 'drug': 1059, 'dubai': 1060, 'eagerly': 1061, 'effort': 1062, 'encourage': 1063, 'exposed': 1064, 'extent': 1065, 'eyes': 1066, 'fall': 1067, 'families': 1068, 'fear': 1069, 'feminazi': 1070, 'fire': 1071, 'fool': 1072, 'forces': 1073, 'form': 1074, 'front': 1075, 'fucked': 1076, 'gangs': 1077, 'genius': 1078, 'girlfriend': 1079, 'goal': 1080, 'goi': 1081, 'goodness': 1082, 'habit': 1083, 'has': 1084, 'head': 1085, 'heart': 1086, 'holy': 1087, 'homo': 1088, 'hours': 1089, 'hurt': 1090, 'independent': 1091, 'influence': 1092, 'inspired': 1093, 'inspiring': 1094, 'intelligent': 1095, 'interesting': 1096, 'j2j5sssp5yq': 1097, 'judging': 1098, 'justice': 1099, 'k': 1100, 'kapil': 1101, 'kick': 1102, 'kids': 1103, 'killing': 1104, 'lack': 1105, 'land': 1106, 'late': 1107, 'look': 1108, 'loose': 1109, 'lunatic': 1110, 'madarchod': 1111, 'mature': 1112, 'meet': 1113, 'min': 1114, 'mindset': 1115, 'minutes': 1116, 'misogynist': 1117, 'morning': 1118, 'movi': 1119, 'oppose': 1120, 'option': 1121, 'other': 1122, 'pander': 1123, 'parts': 1124, 'path': 1125, 'persons': 1126, 'piece': 1127, 'pit': 1128, 'pity': 1129, 'planet': 1130, 'playing': 1131, 'pro': 1132, 'progress': 1133, 'proof': 1134, 'prove': 1135, 'provide': 1136, 'pure': 1137, 'putting': 1138, 'ram': 1139, 'randy': 1140, 'raped': 1141, 'realised': 1142, 'related': 1143, 'religious': 1144, 'rich': 1145, 'room': 1146, 'ruining': 1147, 'rule': 1148, 'sail': 1149, 'sane': 1150, 'sara': 1151, 'savage': 1152, 'say': 1153, 'saying': 1154, 'see': 1155, 'sell': 1156, 'shallow': 1157, 'sides': 1158, 'silent': 1159, 'sing': 1160, 'slaps': 1161, 'soldiers': 1162, 'someone': 1163, 'spoke': 1164, 'still': 1165, 'students': 1166, 'studying': 1167, 'supported': 1168, 'surprised': 1169, 'taught': 1170, 'terrorism': 1171, 'them': 1172, 'thing': 1173, 'though': 1174, 'three': 1175, 'throw': 1176, 'toilets': 1177, 'topics': 1178, 'transgenes': 1179, 'unable': 1180, 've': 1181, 'vedic': 1182, 'version': 1183, 'victim': 1184, 'vipul': 1185, 'want': 1186, 'white': 1187, 'womens': 1188, 'worse': 1189, 'yeah': 1190, 'yess': 1191, '‡¶æ': 1192, 'accused': 1193, 'actress': 1194, 'add': 1195, 'affect': 1196, 'agar': 1197, 'aggression': 1198, 'air': 1199, 'akshay': 1200, 'allah': 1201, 'allowing': 1202, 'always': 1203, 'amazon': 1204, 'amit': 1205, 'anyone': 1206, 'apparently': 1207, 'apu': 1208, 'armed': 1209, 'arms': 1210, 'aspect': 1211, 'assam': 1212, 'assay': 1213, 'attention': 1214, 'aware': 1215, 'background': 1216, 'banana': 1217, 'before': 1218, 'beggar': 1219, 'benefit': 1220, 'bengal': 1221, 'bhi': 1222, 'bigger': 1223, 'biography': 1224, 'birth': 1225, 'blind': 1226, 'bob': 1227, 'bothered': 1228, 'brave': 1229, 'british': 1230, 'broo': 1231, 'brow': 1232, 'buy': 1233, 'camera': 1234, 'canada': 1235, 'captain': 1236, 'career': 1237, 'catch': 1238, 'chance': 1239, 'changes': 1240, 'channels': 1241, 'civilization': 1242, 'climax': 1243, 'coll': 1244, 'commenting': 1245, 'commission': 1246, 'commit': 1247, 'complex': 1248, 'conduct': 1249, 'conjuring': 1250, 'conservative': 1251, 'considered': 1252, 'contents': 1253, 'convey': 1254, 'count': 1255, 'counter': 1256, 'crimes': 1257, 'critic': 1258, 'critical': 1259, 'criticised': 1260, 'cry': 1261, 'crying': 1262, 'cultural': 1263, 'cunning': 1264, 'daisy': 1265, 'damage': 1266, 'dass': 1267, 'data': 1268, 'date': 1269, 'daughter': 1270, 'david': 1271, 'deal': 1272, 'decided': 1273, 'defence': 1274, 'definition': 1275, 'degree': 1276, 'demo': 1277, 'designer': 1278, 'despite': 1279, 'destruction': 1280, 'dil': 1281, 'directed': 1282, 'discrimination': 1283, 'dislike': 1284, 'dnt': 1285, 'drink': 1286, 'dump': 1287, 'earlier': 1288, 'ect': 1289, 'educated': 1290, 'ego': 1291, 'encounter': 1292, 'ended': 1293, 'epic': 1294, 'etc': 1295, 'everyday': 1296, 'exist': 1297, 'expected': 1298, 'experience': 1299, 'explained': 1300, 'expression': 1301, 'extremely': 1302, 'far': 1303, 'favourite': 1304, 'feels': 1305, 'filled': 1306, 'focus': 1307, 'followers': 1308, 'foolish': 1309, 'forced': 1310, 'frankly': 1311, 'fully': 1312, 'glorify': 1313, 'gon': 1314, 'gud': 1315, 'guts': 1316, 'gym': 1317, 'ha': 1318, 'habits': 1319, 'hahahaha': 1320, 'happily': 1321, 'harm': 1322, 'hatred': 1323, 'heard': 1324, 'history': 1325, 'homophobia': 1326, 'hospital': 1327, 'hostel': 1328, 'httpswww': 1329, 'humana': 1330, 'humanity': 1331, 'hussain': 1332, 'if': 1333, 'ignore': 1334, 'immature': 1335, 'important': 1336, 'indus': 1337, 'interview': 1338, 'intolerant': 1339, 'irrespective': 1340, 'ishtar': 1341, 'kasthuri': 1342, 'khurana': 1343, 'kid': 1344, 'kinda': 1345, 'king': 1346, 'knight': 1347, 'knowing': 1348, 'koi': 1349, 'kolkata': 1350, 'kota': 1351, 'l': 1352, 'lal': 1353, 'lgbtq': 1354, 'liar': 1355, 'light': 1356, 'log': 1357, 'lovers': 1358, 'ltq': 1359, 'maa': 1360, 'madness': 1361, 'mahatma': 1362, 'mai': 1363, 'main': 1364, 'makers': 1365, 'mangal': 1366, 'mard': 1367, 'mart': 1368, 'masala': 1369, 'masses': 1370, 'master': 1371, 'matata': 1372, 'meaningful': 1373, 'mentioned': 1374, 'menus': 1375, 'minister': 1376, 'ministry': 1377, 'mirza': 1378, 'misogynistic': 1379, 'moment': 1380, 'morality': 1381, 'motive': 1382, 'movement': 1383, 'much': 1384, 'mute': 1385, 'mutual': 1386, 'names': 1387, 'nationalism': 1388, 'naval': 1389, 'nazi': 1390, 'negativity': 1391, 'nic': 1392, 'nicely': 1393, 'nil': 1394, 'nobody': 1395, 'note': 1396, 'nowadays': 1397, 'object': 1398, 'original': 1399, 'over': 1400, 'papal': 1401, 'park': 1402, 'pass': 1403, 'passionate': 1404, 'past': 1405, 'patriarchy': 1406, 'period': 1407, 'platform': 1408, 'plays': 1409, 'popular': 1410, 'porn': 1411, 'portrayed': 1412, 'post': 1413, 'practical': 1414, 'praising': 1415, 'prem': 1416, 'presented': 1417, 'press': 1418, 'previous': 1419, 'prime': 1420, 'probably': 1421, 'profession': 1422, 'properly': 1423, 'property': 1424, 'provoke': 1425, 'rage': 1426, 'rahul': 1427, 'raise': 1428, 'rajasthan': 1429, 'ramadhan': 1430, 'ramadhir': 1431, 'raoul': 1432, 'rated': 1433, 'recent': 1434, 'red': 1435, 'relation': 1436, 'relationships': 1437, 'republic': 1438, 'result': 1439, 'romantic': 1440, 'romeo': 1441, 'royal': 1442, 'sake': 1443, 'sam': 1444, 'sanga': 1445, 'sarah': 1446, 'saving': 1447, 'science': 1448, 'security': 1449, 'sedition': 1450, 'seek': 1451, 'seems': 1452, 'sells': 1453, 'sen': 1454, 'senseless': 1455, 'services': 1456, 'sexy': 1457, 'shades': 1458, 'shoot': 1459, 'should': 1460, 'shout': 1461, 'six': 1462, 'slave': 1463, 'smoker': 1464, 'smoking': 1465, 'soft': 1466, 'sold': 1467, 'soon': 1468, 'spirit': 1469, 'spoil': 1470, 'squad': 1471, 'sri': 1472, 'standards': 1473, 'star': 1474, 'statement': 1475, 'statements': 1476, 'storyline': 1477, 'strange': 1478, 'strong': 1479, 'style': 1480, 'subscriber': 1481, 'subtitles': 1482, 'success': 1483, 'suffering': 1484, 'supposed': 1485, 'swara': 1486, 'target': 1487, 'tells': 1488, 'terror': 1489, 'theatre': 1490, 'therapy': 1491, 'thora': 1492, 'thrown': 1493, 'tight': 1494, 'todays': 1495, 'traitors': 1496, 'transcended': 1497, 'treat': 1498, 'treating': 1499, 'trending': 1500, 'trust': 1501, 'tube': 1502, 'turn': 1503, 'understands': 1504, 'unrealistic': 1505, 'uri': 1506, 'us': 1507, 'useless': 1508, 'vanga': 1509, 'vaun': 1510, 'victimhood': 1511, 'vile': 1512, 'villain': 1513, 'wacko': 1514, 'wait': 1515, 'wali': 1516, 'wall': 1517, 'wasseypur': 1518, 'wat': 1519, 'wen': 1520, 'were': 1521, 'womanizer': 1522, 'worked': 1523, 'working': 1524, 'would': 1525, 'writing': 1526, 'xxx': 1527, 'xyz': 1528, 'yas': 1529, 'youngsters': 1530, 'zindabad': 1531, '‡§ø': 1532, '‡¶á': 1533, '‡ßá': 1534, \"'d\": 1535, '20': 1536, '2019': 1537, '910': 1538, 'a4': 1539, 'accusations': 1540, 'achieve': 1541, 'activist': 1542, 'activity': 1543, 'addicted': 1544, 'addiction': 1545, 'admire': 1546, 'advice': 1547, 'afraid': 1548, 'ali': 1549, 'allegations': 1550, 'alpha': 1551, 'american': 1552, 'analyze': 1553, 'angle': 1554, 'animal': 1555, 'anna': 1556, 'anurag': 1557, 'anything': 1558, 'approach': 1559, 'article': 1560, 'asia': 1561, 'ask': 1562, 'aspects': 1563, 'assholes': 1564, 'attracted': 1565, 'attractive': 1566, 'aura': 1567, 'authorities': 1568, 'average': 1569, 'awaiting': 1570, 'band': 1571, 'bangladeshi': 1572, 'bar': 1573, 'barat': 1574, 'bashed': 1575, 'be': 1576, 'beating': 1577, 'beautifully': 1578, 'bec': 1579, 'become': 1580, 'been': 1581, 'beginning': 1582, 'begins': 1583, 'behalf': 1584, 'bengali': 1585, 'bet': 1586, 'beta': 1587, 'bhakts': 1588, 'biased': 1589, 'bitter': 1590, 'blame': 1591, 'blaming': 1592, 'blow': 1593, 'boats': 1594, 'boring': 1595, 'both': 1596, 'botha': 1597, 'boyfriend': 1598, 'brains': 1599, 'breaking': 1600, 'bright': 1601, 'bringing': 1602, 'brothers': 1603, 'cab': 1604, 'careful': 1605, 'cash': 1606, 'cause': 1607, 'celebrated': 1608, 'charged': 1609, 'chasing': 1610, 'cheering': 1611, 'choose': 1612, 'chores': 1613, 'choudhury': 1614, 'chu': 1615, 'citizen': 1616, 'civil': 1617, 'clever': 1618, 'comfortable': 1619, 'commercial': 1620, 'commitment': 1621, 'compassion': 1622, 'concerned': 1623, 'condemn': 1624, 'condition': 1625, 'context': 1626, 'contributed': 1627, 'controversy': 1628, 'couple': 1629, 'couples': 1630, 'cousin': 1631, 'creating': 1632, 'creator': 1633, 'critically': 1634, 'criticism': 1635, 'cult': 1636, 'cup': 1637, 'd': 1638, 'dad': 1639, 'daily': 1640, 'dala': 1641, 'dance': 1642, 'dard': 1643, 'daring': 1644, 'darkness': 1645, 'debating': 1646, 'decade': 1647, 'decisions': 1648, 'deeply': 1649, 'defending': 1650, 'demons': 1651, 'deported': 1652, 'depressed': 1653, 'depth': 1654, 'deserved': 1655, 'destroy': 1656, 'details': 1657, 'dictionary': 1658, 'discussion': 1659, 'disgrace': 1660, 'disturb': 1661, 'dominant': 1662, 'dominating': 1663, 'donate': 1664, 'done': 1665, 'doubt': 1666, 'downfall': 1667, 'duration': 1668, 'duty': 1669, 'e': 1670, 'early': 1671, 'easy': 1672, 'effect': 1673, 'effects': 1674, 'efforts': 1675, 'election': 1676, 'else': 1677, 'elses': 1678, 'empowerment': 1679, 'engaged': 1680, 'enjoyed': 1681, 'episode': 1682, 'escape': 1683, 'eunuch': 1684, 'exaggerated': 1685, 'exercise': 1686, 'exposing': 1687, 'express': 1688, 'extreme': 1689, 'fame': 1690, 'fast': 1691, 'faults': 1692, 'females': 1693, 'fiance': 1694, 'fictional': 1695, 'field': 1696, 'fill': 1697, 'fine': 1698, 'flaws': 1699, 'force': 1700, 'forcefully': 1701, 'four': 1702, 'friendship': 1703, 'fringe': 1704, 'fucks': 1705, 'funds': 1706, 'gali': 1707, 'gem': 1708, 'general': 1709, 'genuinely': 1710, 'gods': 1711, 'goes': 1712, 'goole': 1713, 'granted': 1714, 'groups': 1715, 'guilty': 1716, 'gulf': 1717, 'gully': 1718, 'gulo': 1719, 'harass': 1720, 'haryana': 1721, 'hasan': 1722, 'hating': 1723, 'heavy': 1724, 'helped': 1725, 'hi': 1726, 'highly': 1727, 'hindustan': 1728, 'ho': 1729, 'hup': 1730, 'hypocrites': 1731, 'idolized': 1732, 'iii': 1733, 'immediate': 1734, 'impose': 1735, 'incidents': 1736, 'individuals': 1737, 'indulge': 1738, 'infection': 1739, 'instigate': 1740, 'instigated': 1741, 'instigating': 1742, 'insult': 1743, 'intense': 1744, 'interstellar': 1745, 'intrest': 1746, 'involved': 1747, 'jailed': 1748, 'jesus': 1749, 'jobs': 1750, 'journey': 1751, 'kalkara': 1752, 'kedarnath': 1753, 'keeping': 1754, 'kgb': 1755, 'kgf': 1756, 'kill': 1757, 'kindly': 1758, 'kissed': 1759, 'kohl': 1760, 'kotha': 1761, 'kudos': 1762, 'kutiya': 1763, 'kutti': 1764, 'last': 1765, 'laura': 1766, 'leader': 1767, 'leaves': 1768, 'leaving': 1769, 'less': 1770, 'license': 1771, 'lied': 1772, 'lifestyle': 1773, 'logo': 1774, 'lok': 1775, 'looks': 1776, 'looser': 1777, 'low': 1778, 'loyal': 1779, 'lryevxeva': 1780, 'lying': 1781, 'machu': 1782, 'majority': 1783, 'males': 1784, 'management': 1785, 'manner': 1786, 'mans': 1787, 'manual': 1788, 'many': 1789, 'mao': 1790, 'marriages': 1791, 'mask': 1792, 'massagboy': 1793, 'matters': 1794, 'medico': 1795, 'mediocre': 1796, 'meets': 1797, 'member': 1798, 'mentality': 1799, 'mention': 1800, 'middle': 1801, 'military': 1802, 'million': 1803, 'mine': 1804, 'mins': 1805, 'minute': 1806, 'mirror': 1807, 'mistake': 1808, 'mm': 1809, 'mondol': 1810, 'month': 1811, 'morally': 1812, 'morals': 1813, 'moves': 1814, 'mud': 1815, 'mumbai': 1816, 'murder': 1817, 'music': 1818, 'n3d5ilngama': 1819, 'narratives': 1820, 'narrow': 1821, 'ncc': 1822, 'next': 1823, 'nine': 1824, 'obnoxious': 1825, 'obsessed': 1826, 'obsession': 1827, 'offensive': 1828, 'on': 1829, 'opener': 1830, 'opening': 1831, 'opportunity': 1832, 'orbit': 1833, 'ord': 1834, 'otherwise': 1835, 'out': 1836, 'pair': 1837, 'par': 1838, 'parliament': 1839, 'party': 1840, 'patriarchal': 1841, 'paul': 1842, 'phase': 1843, 'phone': 1844, 'photon': 1845, 'pig': 1846, 'plot': 1847, 'porimoni': 1848, 'potray': 1849, 'practice': 1850, 'praised': 1851, 'pratley': 1852, 'pregnant': 1853, 'pressure': 1854, 'priority': 1855, 'problematic': 1856, 'producers': 1857, 'projects': 1858, 'pros': 1859, 'prosecuted': 1860, 'psychological': 1861, 'pyar': 1862, 'q': 1863, 'questions': 1864, 'quick': 1865, 'race': 1866, 'radhe': 1867, 'rajeev': 1868, 'rajesh': 1869, 'rajiv': 1870, 'rather': 1871, 'reactions': 1872, 'realise': 1873, 'redemption': 1874, 'regressive': 1875, 'relating': 1876, 'release': 1877, 'relevant': 1878, 'remain': 1879, 'remove': 1880, 'replicate': 1881, 'report': 1882, 'research': 1883, 'respectable': 1884, 'respected': 1885, 'response': 1886, 'responsibilities': 1887, 'reviewing': 1888, 'ridiculous': 1889, 'rise': 1890, 'roles': 1891, 'rooted': 1892, 'rotten': 1893, 'ruined': 1894, 'rules': 1895, 'running': 1896, 'saal': 1897, 'sahara': 1898, 'sarcastic': 1899, 'saved': 1900, 'says': 1901, 'scared': 1902, 'script': 1903, 'selective': 1904, 'set': 1905, 'sexism': 1906, 'sexually': 1907, 'sh': 1908, 'shameful': 1909, 'sheds': 1910, 'shirts': 1911, 'shocked': 1912, 'similar': 1913, 'sin': 1914, 'singles': 1915, 'sisters': 1916, 'sites': 1917, 'sitting': 1918, 'skills': 1919, 'sleep': 1920, 'sleeping': 1921, 'small': 1922, 'solely': 1923, 'son': 1924, 'sonar': 1925, 'source': 1926, 'special': 1927, 'specialist': 1928, 'specifically': 1929, 'spoilers': 1930, 'spoken': 1931, 'spotted': 1932, 'stage': 1933, 'stating': 1934, 'station': 1935, 'stick': 1936, 'strongly': 1937, 'studied': 1938, 'studies': 1939, 'stunt': 1940, 'subjected': 1941, 'subscribe': 1942, 'substance': 1943, 'sucked': 1944, 'supporter': 1945, 'supports': 1946, 'surgery': 1947, 'suu': 1948, 'sweet': 1949, 'teacher': 1950, 'technology': 1951, 'terrible': 1952, 'terrorist': 1953, 'th': 1954, 'than': 1955, 'thanked': 1956, 'theatres': 1957, 'thier': 1958, 'thousands': 1959, 'threatening': 1960, 'tht': 1961, 'tiara': 1962, 'toilet': 1963, 'tolerate': 1964, 'tom': 1965, 'treated': 1966, 'treats': 1967, 'truly': 1968, 'tutti': 1969, 'two': 1970, 'types': 1971, 'ultimate': 1972, 'ultra': 1973, 'uncomfortable': 1974, 'understood': 1975, 'uneducated': 1976, 'unlike': 1977, 'unnecessary': 1978, 'upcoming': 1979, 'usher': 1980, 'victims': 1981, 'viewer': 1982, 'villa': 1983, 'villan': 1984, 'vulgar': 1985, 'walk': 1986, 'wasted': 1987, 'wasting': 1988, 'ways': 1989, 'weak': 1990, 'wear': 1991, 'web': 1992, 'webster': 1993, 'weird': 1994, 'wichita': 1995, 'wid': 1996, 'witch': 1997, 'writer': 1998, 'writers': 1999, 'wrote': 2000, 'yellow': 2001, 'youths': 2002, 'zaid': 2003, '‡§≤': 2004, '‡¶ü': 2005, '‡¶®': 2006, '‡¶≠': 2007, 'ü§ò': 2008, '\\U0001f91f': 2009, 'ü§†': 2010, '\\U0001f970': 2011})\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data,min_freq=3, vectors = vectors)  \n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(\"Size of topic vocab:\",len(TEXT.vocab))\n",
    "print(\"Size of label vocab:\",len(LABEL.vocab))\n",
    "print(TEXT.vocab.freqs.most_common(11))  \n",
    "print(LABEL.vocab.freqs.most_common(14))\n",
    "print(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = len(LABEL.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ramanshgrover/anaconda3/lib/python3.6/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "\n",
    "b_sz = 128\n",
    "\n",
    "train_loader, val_loader = data.BucketIterator.splits(\n",
    "    (train_data, valid_data),\n",
    "    batch_size = b_sz,\n",
    "    sort_key = lambda x: len(x.monolingual),\n",
    "    sort_within_batch=True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim = 12, n_layers = 2, bidir = True, dropout = 0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidir,\n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        self.dense = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        embedded = self.embedding(text)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        dense_outputs=self.dense(hidden)\n",
    "        outputs=self.softmax(dense_outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(TEXT.vocab)\n",
    "embedding_dim = 300\n",
    "num_hidden_nodes = 32\n",
    "num_output_nodes = labels\n",
    "dropout = 0.2\n",
    "\n",
    "model_ = model(vocab_size, embedding_dim, num_hidden_nodes, num_output_nodes, dropout = dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model(\n",
      "  (embedding): Embedding(2012, 300)\n",
      "  (lstm): LSTM(300, 32, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (dense): Linear(in_features=64, out_features=3, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n",
      "The model has 714,387 trainable parameters\n",
      "torch.Size([2012, 300])\n"
     ]
    }
   ],
   "source": [
    "print(model_)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "print(f'The model has {count_parameters(model_):,} trainable parameters')\n",
    "\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model_.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model_.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def accuracy(preds, y):\n",
    "    counts = 0\n",
    "    for i in range(preds.shape[0]):\n",
    "      counts += (torch.max(preds[i], 0)[1] == y[i]).float()\n",
    "      \n",
    "    return counts/preds.shape[0]\n",
    "    \n",
    "model_ = model_.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()  \n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        text, text_lengths = batch.monolingual\n",
    "        label = batch.label\n",
    "\n",
    "        text = text.to(device)\n",
    "        label = label.type(torch.LongTensor).to(device)\n",
    "\n",
    "        predictions = model(text, text_lengths).squeeze()\n",
    "\n",
    "        try:\n",
    "              loss = criterion(predictions, label)\n",
    "              acc = accuracy(predictions, label)\n",
    "        except:\n",
    "              continue\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()  \n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / (len(iterator)-1), epoch_acc / (len(iterator)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "        \n",
    "            text, text_lengths = batch.monolingual\n",
    "            label = batch.label\n",
    "\n",
    "            text = text.to(device)\n",
    "            label = label.type(torch.LongTensor).to(device)\n",
    "            \n",
    "            predictions = model(text, text_lengths).squeeze()\n",
    "            \n",
    "            try:\n",
    "                  loss = criterion(predictions, label)\n",
    "                  acc = accuracy(predictions, label)\n",
    "            except:\n",
    "                  continue\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / (len(iterator)-1), epoch_acc / (len(iterator)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ramanshgrover/anaconda3/lib/python3.6/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/Users/ramanshgrover/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a55186d25eca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-94a0cf9b2950>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-9577b7a28516>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text, text_lengths)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mpacked_embedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mpacked_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_embedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first, enforce_sorted)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_packed_sequence_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "     \n",
    "    train_loss, train_acc = train(model_, train_loader, optimizer, criterion)\n",
    "    \n",
    "    valid_loss, valid_acc = evaluate(model_, val_loader, criterion)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc:.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "      optimizer.zero_grad()   \n",
    "      \n",
    "      text, text_lengths = batch.monolingual   \n",
    "      \n",
    "      predictions = model_(text, text_lengths).squeeze()\n",
    "      loss = criterion(predictions, batch.label.type(torch.LongTensor))\n",
    "      \n",
    "      loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchnlp\n",
    "from torchnlp.encoders import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "      <th>B</th>\n",
       "      <th>clean</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>normalized lexicon</th>\n",
       "      <th>monolingual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>C45.451</td>\n",
       "      <td>Next part</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "      <td>next part</td>\n",
       "      <td>['next', 'part']</td>\n",
       "      <td>['next', 'part']</td>\n",
       "      <td>next part</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>C47.11</td>\n",
       "      <td>Iii8mllllllm\\nMdxfvb8o90lplppi0005</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "      <td>iii8mllllllm mdxfvb8o90lplppi0005</td>\n",
       "      <td>['iii', '8mllllllm', 'mdxfvb', '8o90lplppi0005']</td>\n",
       "      <td>['iii', '8mllllllm', 'mdxfvb', '8o90lplppi0005']</td>\n",
       "      <td>iii 8mllllllm mdxfvb 8o90lplppi0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>C33.79</td>\n",
       "      <td>ü§£ü§£üòÇüòÇü§£ü§£ü§£üòÇosm vedio ....keep it up...make more v...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "      <td>osm vedio make vedios</td>\n",
       "      <td>['osm', 'vedio', 'make', 'vedios']</td>\n",
       "      <td>['osm', 'osf', 'vedic', 'make', 'videos']</td>\n",
       "      <td>osm osf vedic make videos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>C4.1961</td>\n",
       "      <td>What the fuck was this? I respect shwetabh and...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "      <td>what fuck this? respect shwetabh watching vide...</td>\n",
       "      <td>['what', 'fuck', 'this', '?', 'respect', 'shwe...</td>\n",
       "      <td>['what', 'fuck', 'fuck', 'this', '?', 'respect...</td>\n",
       "      <td>what fuck fuck this ? respect respect whitish ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>C10.153</td>\n",
       "      <td>Concerned authorities should bring arundathi R...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "      <td>concerned authorities bring arundathi roy type...</td>\n",
       "      <td>['concerned', 'authorities', 'bring', 'arundat...</td>\n",
       "      <td>['concerned', 'authorities', 'bring', 'arundat...</td>\n",
       "      <td>concerned authorities bring arundathi roy roy ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       ID                                               Text  \\\n",
       "0           0  C45.451                                          Next part   \n",
       "1           1   C47.11                 Iii8mllllllm\\nMdxfvb8o90lplppi0005   \n",
       "2           2   C33.79  ü§£ü§£üòÇüòÇü§£ü§£ü§£üòÇosm vedio ....keep it up...make more v...   \n",
       "3           3  C4.1961  What the fuck was this? I respect shwetabh and...   \n",
       "4           4  C10.153  Concerned authorities should bring arundathi R...   \n",
       "\n",
       "  label     B                                              clean  \\\n",
       "0   NAG  NGEN                                          next part   \n",
       "1   NAG  NGEN                  iii8mllllllm mdxfvb8o90lplppi0005   \n",
       "2   NAG  NGEN                              osm vedio make vedios   \n",
       "3   NAG  NGEN  what fuck this? respect shwetabh watching vide...   \n",
       "4   NAG  NGEN  concerned authorities bring arundathi roy type...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0                                   ['next', 'part']   \n",
       "1   ['iii', '8mllllllm', 'mdxfvb', '8o90lplppi0005']   \n",
       "2                 ['osm', 'vedio', 'make', 'vedios']   \n",
       "3  ['what', 'fuck', 'this', '?', 'respect', 'shwe...   \n",
       "4  ['concerned', 'authorities', 'bring', 'arundat...   \n",
       "\n",
       "                                  normalized lexicon  \\\n",
       "0                                   ['next', 'part']   \n",
       "1   ['iii', '8mllllllm', 'mdxfvb', '8o90lplppi0005']   \n",
       "2          ['osm', 'osf', 'vedic', 'make', 'videos']   \n",
       "3  ['what', 'fuck', 'fuck', 'this', '?', 'respect...   \n",
       "4  ['concerned', 'authorities', 'bring', 'arundat...   \n",
       "\n",
       "                                         monolingual  \n",
       "0                                          next part  \n",
       "1                iii 8mllllllm mdxfvb 8o90lplppi0005  \n",
       "2                          osm osf vedic make videos  \n",
       "3  what fuck fuck this ? respect respect whitish ...  \n",
       "4  concerned authorities bring arundathi roy roy ...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    z = np.zeros(12, dtype = int)\n",
    "    z[x-1] = 1\n",
    "    return tuple(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, label = 'label', text = 'monolingual', drop = ['Unnamed: 0', 'ID','Text','B','clean','tokenized','normalized lexicon'], train = False):\n",
    "    df_s = df\n",
    "    df_s['text'] = ''\n",
    "    for sent in df[text]:\n",
    "        df_s['text'] += sent\n",
    "        df_s['text'] += \" \"\n",
    "    df_s['labels'] = pd.Series(encoder.batch_encode(list(df[label]))).apply(func)\n",
    "    df_s = df_s.drop(drop, 'columns')\n",
    "    train_df, eval_df = train_test_split(df_s, test_size=0.2)\n",
    "\n",
    "    if(train):\n",
    "        return train_df, eval_df\n",
    "    else:\n",
    "        return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, eval_df = prepare_data(df, label = 'label', text = 'monolingual', train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = eval_df.reset_index()\n",
    "train_df = train_df.reset_index()\n",
    "eval_df = eval_df.drop('index', 'columns')\n",
    "train_df = train_df.drop('index', 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"output_dir\": \"outputs/\",\n",
    "    \"cache_dir\": \"cache/\",\n",
    "    \"best_model_dir\": \"outputs/best_model/\",\n",
    "\n",
    "    \"fp16\": False,\n",
    "    \"fp16_opt_level\": \"O1\",\n",
    "    \"max_seq_length\": 128,\n",
    "    \"train_batch_size\": 128,\n",
    "    \"eval_batch_size\": 128,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"weight_decay\": 0,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"warmup_ratio\": 0.06,\n",
    "    \"warmup_steps\": 0,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"do_lower_case\": False,\n",
    "\n",
    "    \"logging_steps\": 50,\n",
    "    \"evaluate_during_training\": False,\n",
    "    \"evaluate_during_training_steps\": 2000,\n",
    "    \"evaluate_during_training_verbose\": False,\n",
    "    \"use_cached_eval_features\": False,\n",
    "    \"save_eval_checkpoints\": True,\n",
    "    \"no_cache\": False,\n",
    "    \"save_model_every_epoch\": True,\n",
    "    \"tensorboard_dir\": None,\n",
    "\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"reprocess_input_data\": True,\n",
    "\n",
    "    \"process_count\": cpu_count() - 2 if cpu_count() > 2 else 1,\n",
    "    \"n_gpu\": 1,\n",
    "    \"silent\": False,\n",
    "    \"use_multiprocessing\": True,\n",
    "\n",
    "    \"wandb_project\": None,\n",
    "    \"wandb_kwargs\": {},\n",
    "\n",
    "    \"use_early_stopping\": True,\n",
    "    \"early_stopping_patience\": 3,\n",
    "    \"early_stopping_delta\": 0,\n",
    "    \"early_stopping_metric\": \"eval_loss\",\n",
    "    \"early_stopping_metric_minimize\": True,\n",
    "\n",
    "    \"manual_seed\": None,\n",
    "    \"encoding\": None,\n",
    "    \"config\": {},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import MultiLabelClassificationModel\n",
    "\n",
    "model = MultiLabelClassificationModel('distilbert', 'distilbert-base-cased', num_labels=12, args = args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_model(train_df, show_running_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, model_outputs, wrong_predictions = model.eval_model(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(eval_df.shape[0])\n",
    "print(encoder.decode(torch.tensor(model_outputs[i].argmax()+1)))\n",
    "print(eval_df['text'].values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r /content/outputs/checkpoint-5226-epoch-2 ./Cache/Models/DistilBERT/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import MultiLabelClassificationModel\n",
    "\n",
    "model = MultiLabelClassificationModel('distilbert', 'distilbert-base-uncased', num_labels=24, args = args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_model(train_df, show_running_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, model_outputs, wrong_predictions = model.eval_model(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(eval_df.shape[0])\n",
    "print(encoder.decode(torch.tensor(model_outputs[i].argmax()+1)))\n",
    "print(eval_df['text'].values[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import MultiLabelClassificationModel\n",
    "\n",
    "model = MultiLabelClassificationModel('roberta', 'distilroberta-base', num_labels=12, args = args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_model(train_df, show_running_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, model_outputs, wrong_predictions = model.eval_model(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(eval_df.shape[0])\n",
    "print(encoder.decode(torch.tensor(model_outputs[i].argmax()+1)))\n",
    "print(eval_df['text'].values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './Cache/Models/RoBERTA/model_roberta_4.h5')\n",
    "torch.save(model.state_dict(), './Cache/Models/RoBERTA/model_roberta_6.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(eval_df.shape[0])\n",
    "print(encoder.decode(torch.tensor(model_outputs[i].argmax()+1)))\n",
    "print(eval_df['text'].values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r /content/outputs/checkpoint-2613-epoch-1 ./Cache/Models/DistilBERT/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import MultiLabelClassificationModel\n",
    "\n",
    "model = MultiLabelClassificationModel('albert', 'albert-large-v1', num_labels=12, args = args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_model(train_df, show_running_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, model_outputs, wrong_predictions = model.eval_model(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def roc_curve(y_score=model_outputs, y_test=y_test, n_classes = 12, col=2):\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "for col in range(12):\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr[col], tpr[col], color='darkorange',\n",
    "          lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[col])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Flair: ' + str(encoder.decode(torch.tensor(col+1))))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('./Cache/Figures/DistilBERT/' + str(col+1) + '.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
